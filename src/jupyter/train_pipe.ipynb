{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3cbac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hao/.pyenv/versions/3.6.15/envs/torch-cpu5/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106227180>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from hydra import initialize, compose\n",
    "import pathlib\n",
    "# import config\n",
    "\n",
    "import data_process.neg_sample as ng_sample\n",
    "from data_process.utils import mix_merge\n",
    "from data_process.data_split import data_split_user\n",
    "from evaluate_entity import CustomHR, CustomNDCG, CustomRoc, CustomRoctop, CustomRecall_top, CustomPrecision_top\n",
    "from model_entity import EntityCat\n",
    "from data_utils import CatData\n",
    "from utils.constants import DEFAULT_USER_COL,DEFAULT_ITEM_COL,DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL\n",
    "\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score, precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import argparse\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452234bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator, RemovableEventHandle\n",
    "from ignite.metrics import Accuracy, Loss, Metric\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.exceptions import NotComputableError\n",
    "from ignite.metrics.metric import sync_all_reduce, reinit__is_reduced\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "# from ignite.contrib.handlers import TensorboardLogger \n",
    "from ignite.contrib.handlers.wandb_logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c9d49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c783c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3fca5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type =='cpu':\n",
    "    BATCH_SIZE = cfg.params.batch_size_cpu\n",
    "    EPOCHS  = cfg.params.epochs_cpu\n",
    "else:\n",
    "    BATCH_SIZE = cfg.params.batch_size_gpu\n",
    "    EPOCHS  = cfg.params.epochs_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e094616",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cpu':\n",
    "    use_amp=False\n",
    "    df_train_pos  = ng_sample.read_feather(pathlib.Path(cfg.path.root, cfg.file.train_pos))\n",
    "    df_train_neg = pd.read_feather(pathlib.Path(cfg.path.root, cfg.file.train_neg))\n",
    "    df_test_ori = pd.read_feather(pathlib.Path(cfg.path.root, cfg.file.test)).iloc[:202,]\n",
    "    df_all_features = pd.read_csv(pathlib.Path(cfg.path.root, cfg.file.all_features))\n",
    "    df_train_pos = df_train_pos.sort_values(by=[DEFAULT_USER_COL]).iloc[:100,].reset_index(drop=True)\n",
    "    df_train_neg = df_train_neg.sort_values(by=[DEFAULT_USER_COL]).iloc[:100*cfg.params.neg_train,].reset_index(drop=True)\n",
    "else:\n",
    "    use_amp=True\n",
    "    df_train_pos  = ng_sample.read_feather(pathlib.Path(cfg.path.root, cfg.file.train_pos))\n",
    "    df_train_neg = pd.read_feather(pathlib.Path(cfg.path.root, cfg.file.train_neg))\n",
    "    df_test_ori = pd.read_feather(pathlib.Path(cfg.path.root, cfg.file.test))\n",
    "    df_all_features = pd.read_csv(pathlib.Path(cfg.path.root, cfg.file.all_features))\n",
    "    df_train_pos = df_train_pos.sort_values(by=[DEFAULT_USER_COL]).reset_index(drop=True)\n",
    "    df_train_neg = df_train_neg.sort_values(by=[DEFAULT_USER_COL]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97918f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos[DEFAULT_RATING_COL] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b8c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_index(df1, df2):\n",
    "    df2.index = df2.index//cfg.params.neg_train\n",
    "    return pd.concat([df1, df2], axis=0).sort_index(kind='mregesort').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103e06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all = concat_index(df_train_pos, df_train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9255ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['flag'] = 1\n",
    "df_test_ori['flag'] = 0\n",
    "df_all = pd.concat([df_train_all, df_test_ori], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc6348",
   "metadata": {},
   "source": [
    "user features: \n",
    "       'WindowID_user', 'Split', 'City',\n",
    "       'State', 'Country', 'Zip_user', 'DegreeType', 'Major', 'GraduationDate',\n",
    "       'WorkHistoryCount', 'TotalYearsExperience', 'CurrentlyEmployed',\n",
    "       'ManagedOthers', 'ManagedHowMany',\n",
    "       \n",
    "job features: \n",
    "       'WindowID_job', 'City_job',\n",
    "       'State_job', 'Country_job', 'Zip_job', 'StartDate', 'EndDate',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715af10",
   "metadata": {},
   "source": [
    "### Choose the features and process data for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939ed0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userid', 'itemid', 'rating', 'flag'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a67e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['City']\n",
    "user_features_extend = [DEFAULT_USER_COL] + user_features\n",
    "\n",
    "item_features = ['City_job']\n",
    "item_features_extend =[DEFAULT_ITEM_COL] + item_features\n",
    "\n",
    "base_features = [DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0be3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix_merge = mix_merge(df_all , df_all_features, user_features_extend, item_features_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8084733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cat_encode(df_data, list_f, encoder):\n",
    "    for f in list_f:\n",
    "        df_data[f] = encoder.fit_transform(df_data[f].astype('category').cat.codes.values)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f75839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embedding_dimension(df_all_encode, features_to_train, max_dim=50):\n",
    "\n",
    "    embedding_size = []\n",
    "    features_to_em = [i for i in features_to_train if i !=DEFAULT_RATING_COL]\n",
    "    for c in features_to_em:\n",
    "        num_unique_values = int(df_all_encode[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(num_unique_values/2), max_dim))\n",
    "        embedding_size.append([num_unique_values, embed_dim])  \n",
    "    return embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a26ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df_mix_merge, features_to_code, features_to_train, max_dim=50):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    df_all_encode = _cat_encode(df_mix_merge, features_to_code, encoder)\n",
    "    df_train = df_all_encode[df_all.flag==1]\n",
    "    df_test = df_all_encode[df_all.flag==0]\n",
    "    df_train = df_train[features_to_train]\n",
    "    df_test = df_test[features_to_train]\n",
    "    embedding_size = _embedding_dimension(df_all_encode, features_to_train, max_dim)\n",
    "    return df_train, df_test, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eb03d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature=[]\n",
    "features_to_code = df_mix_merge.columns\n",
    "features_to_train = [DEFAULT_USER_COL, DEFAULT_ITEM_COL]+ user_features + item_features +[DEFAULT_RATING_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9370037a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of embedding layers:[[36, 18], [413, 50], [35, 18], [230, 50]]\n"
     ]
    }
   ],
   "source": [
    "df_train,  df_test, embedding_size = encode_data(df_mix_merge, features_to_code, features_to_train, max_dim=cfg.params.emb_dim)\n",
    "\n",
    "print(f'The size of embedding layers:{embedding_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff86395",
   "metadata": {},
   "source": [
    "## Run data check before training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e8d7b",
   "metadata": {},
   "source": [
    "Check the ratio of positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e951d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train[df_train.rating==0])/len(df_train[df_train.rating==1]) == cfg.params.neg_train, 'wrong neg/pos ratio in training set'\n",
    "assert len(df_test[df_test.rating==0])/len(df_test[df_test.rating==1]) == cfg.params.neg_test, 'wrong neg/pos ratio in test set '\n",
    "#Check if all the users in test can be found in training set\n",
    "assert sum(np.isin(df_test.userid.unique(), df_train.userid.unique(), assume_unique=True)) == len(df_test.userid.unique()), 'cold start'\n",
    "#The the uniqueness of items between training and test. For a user, on common items between training and test dataset. \n",
    "assert df_all.shape[0] ==df_train.shape[0]+df_test.shape[0], 'wrong data concat'\n",
    "assert sum(df_all.groupby(['userid']).apply(lambda x: len(x['itemid'].unique()))) == df_all.shape[0], 'train and test have overlap item'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1f16a",
   "metadata": {},
   "source": [
    "## Creat the numpy array for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98032331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_split, df_val_split = data_split_user(df_train, val_size=0.2)\n",
    "\n",
    "np_train = df_train_split.values\n",
    "np_val = df_val_split.values\n",
    "np_test = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb2a567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x121c4cf78>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb16f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CatData(np_train)\n",
    "val_dataset = CatData(np_val)\n",
    "test_dataset = CatData(np_test) \n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,  worker_init_fn=seed_worker,generator=g)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, num_workers=0,  worker_init_fn=seed_worker,generator=g)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=cfg.params.neg_test+1, shuffle=False, num_workers=0,worker_init_fn=seed_worker,generator=g )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8694e807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntityCat(\n",
       "  (all_embeddings): ModuleList(\n",
       "    (0): Embedding(36, 18)\n",
       "    (1): Embedding(413, 50)\n",
       "    (2): Embedding(35, 18)\n",
       "    (3): Embedding(230, 50)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (mlp_layers): Sequential(\n",
       "    (0): Linear(in_features=136, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (predict_layer): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EntityCat(embedding_size = embedding_size, num_numerical_cols = len(num_feature),\n",
    "               output_size = 2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7098e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.params.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f38e4bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityCat(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(36, 18)\n",
      "    (1): Embedding(413, 50)\n",
      "    (2): Embedding(35, 18)\n",
      "    (3): Embedding(230, 50)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
      "  (mlp_layers): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=100, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (predict_layer): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686edeec",
   "metadata": {},
   "source": [
    "## Model the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9670b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_trans_loss(output):\n",
    "    return output['y_pred'], output['label']\n",
    "\n",
    "val_metrics_train = {\n",
    "    'auc': CustomRoc(),\n",
    "    \"loss\": Loss(criterion, output_transform=output_trans_loss)\n",
    "}\n",
    "\n",
    "val_metrics_test = {\n",
    "    'hr': CustomHR(),\n",
    "    'ndcg': CustomNDCG(),\n",
    "    'auc': CustomRoc(),\n",
    "    'roc_top': CustomRoctop(),\n",
    "    'recall_top': CustomRecall_top(threshold=0.5),\n",
    "    'precision_top': CustomPrecision_top(threshold=0.5),\n",
    "    \"loss\": Loss(criterion, output_transform=output_trans_loss)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df93a20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x12ed7bdd8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch[0].to(device), batch[1].to(device)\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        y_pred = model(x)[:,1]\n",
    "        loss = criterion(y_pred, y.float())\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    return loss.item()\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, label = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)[:,1]\n",
    "        label=label.float()\n",
    "        return {'label':label, 'y_pred':y_pred}\n",
    "\n",
    "def test_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, label = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)[:,1]\n",
    "        label=label.float()\n",
    "        y_pred_top, indices = torch.topk(y_pred, engine.state.topk)\n",
    "        y_pred_top = y_pred_top.detach().cpu().numpy()\n",
    "        reco_item = torch.take(x[:,1], indices).cpu().numpy().tolist()\n",
    "        pos_item = x[0,1].cpu().numpy().tolist()  # ground truth, item id\n",
    "        label_top = label[indices].cpu().numpy()\n",
    "        return {'pos_item':pos_item, 'reco_item':reco_item, 'y_pred_top':y_pred_top, 'label_top':label_top, 'label':label, 'y_pred':y_pred}\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "train_evaluator = Engine(validation_step)\n",
    "# train_evaluator.state_dict_user_keys.append('topk')\n",
    "\n",
    "val_evaluator = Engine(validation_step)\n",
    "# val_evaluator.state_dict_user_keys.append('topk')\n",
    "\n",
    "test_evaluator = Engine(test_step)\n",
    "test_evaluator.state_dict_user_keys.append('topk')\n",
    "\n",
    "# @val_evaluator.on(Events.STARTED)\n",
    "# def init_user_value():\n",
    "#     val_evaluator.state.topk=3\n",
    "    \n",
    "# @train_evaluator.on(Events.STARTED)\n",
    "# def init_user_value():\n",
    "#     train_evaluator.state.topk=3\n",
    "\n",
    "@train_evaluator.on(Events.STARTED)\n",
    "def init_user_value():\n",
    "    test_evaluator.state.topk=cfg.params.topk\n",
    "    \n",
    "    \n",
    "# Attach metrics to the evaluators\n",
    "for name, metric in val_metrics_train.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "\n",
    "for name, metric in val_metrics_train.items():\n",
    "    metric.attach(val_evaluator, name)\n",
    "\n",
    "    \n",
    "for name, metric in val_metrics_test.items():\n",
    "    metric.attach(test_evaluator, name)\n",
    "    \n",
    "# Eearly_stop \n",
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics['auc']\n",
    "    return val_loss\n",
    "\n",
    "Eearly_stop_handler = EarlyStopping(patience=cfg.params.patience, score_function=score_function, trainer=trainer)\n",
    "val_evaluator.add_event_handler(Events.COMPLETED, Eearly_stop_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b9d6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    auc = metrics['auc']\n",
    "    loss = metrics['loss']\n",
    "    print(f'Training Results- Epoch[{trainer.state.epoch}]  Avg loss: {loss:.2f} \\\n",
    "          Avg auc:{auc:.2f}')\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    auc = metrics['auc']\n",
    "    loss = metrics['loss']\n",
    "    print(f'Validation Results- Epoch[{trainer.state.epoch}]  Avg loss: {loss:.2f} \\\n",
    "          Avg auc:{auc:.2f}')\n",
    "\n",
    "    \n",
    "@trainer.on(Events.COMPLETED)\n",
    "def log_test_results(trainer):\n",
    "    test_evaluator.run(test_loader)\n",
    "    metrics = test_evaluator.state.metrics\n",
    "    hr = metrics['hr']\n",
    "    ndcg = metrics['ndcg']\n",
    "    auc = metrics['auc']\n",
    "    roc_top = metrics['roc_top']\n",
    "    recall = metrics['recall_top']\n",
    "    precision = metrics['precision_top']\n",
    "    loss = metrics['loss']\n",
    "    print(f\"Test Results - Epoch[{trainer.state.epoch}]  Avg loss: {loss:.2f} \\\n",
    "     Avg ndcg: {ndcg:.2f}  Avg auc: {auc:.2f}  Avg auc_top: {roc_top:.2f} \\\n",
    "      Avg recall: {recall:.2f}  Avg precision: {precision:.2f}\")\n",
    "\n",
    "pbar = ProgressBar(persist=False)\n",
    "# pbar.attach(trainer)\n",
    "pbar.attach(trainer)\n",
    "# from ignite.handlers import Timer\n",
    "# timer = Timer(average=True)\n",
    "# timer.attach(trainer,\n",
    "#              start=Events.STARTED,\n",
    "#              resume=Events.EPOCH_STARTED,\n",
    "#              pause=Events.EPOCH_COMPLETED,\n",
    "#              step=Events.EPOCH_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af997b32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b18f030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiyuok2023\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hao/Documents/MA_thesis/ncf-torch2/src/jupyter/wandb/run-20220704_134526-1hwiim4f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiyuok2023/pytorch-jrs/runs/1hwiim4f\" target=\"_blank\">City-City_job</a></strong> to <a href=\"https://wandb.ai/tiyuok2023/pytorch-jrs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">City-City_job</strong>: <a href=\"https://wandb.ai/tiyuok2023/pytorch-jrs/runs/1hwiim4f\" target=\"_blank\">https://wandb.ai/tiyuok2023/pytorch-jrs/runs/1hwiim4f</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220704_134526-1hwiim4f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_dict = dict(cfg.params)\n",
    "config_dict['Features']='-'.join(user_features+item_features)\n",
    "wandb_logger = WandBLogger(\n",
    "    project=\"pytorch-jrs\",\n",
    "    name=\"-\".join(user_features)+'-'+'-'.join(item_features),\n",
    "    config=config_dict,\n",
    "    tags=[\"entity\", \"jrs\"]\n",
    ")\n",
    "\n",
    "to_save = {'model': model}\n",
    "checkpoint_handler = ModelCheckpoint(\n",
    "    wandb_logger.run.dir,\n",
    "    n_saved=1, filename_prefix='best',\n",
    "    score_name=\"auc\",\n",
    "    global_step_transform=global_step_from_engine(trainer)\n",
    ")\n",
    "\n",
    "val_evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler, to_save)\n",
    "\n",
    "    \n",
    "wandb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED,\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"loss\": loss}\n",
    ")\n",
    "\n",
    "wandb_logger.attach_output_handler(\n",
    "    train_evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"training\",\n",
    "    metric_names=['loss','auc'],\n",
    "    global_step_transform=lambda *_: trainer.state.iteration,\n",
    ")\n",
    "\n",
    "wandb_logger.attach_output_handler(\n",
    "    val_evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"validation\",\n",
    "    metric_names=['loss',\"auc\"],\n",
    "    global_step_transform=lambda *_: trainer.state.iteration,\n",
    ")\n",
    "\n",
    "\n",
    "wandb_logger.attach_output_handler(\n",
    "    test_evaluator,\n",
    "    event_name=Events.COMPLETED,\n",
    "    tag=\"test\",\n",
    "    metric_names=['loss',\"auc\", 'hr', 'ndcg', 'roc_top', 'recall_top', 'precision_top'],\n",
    "    global_step_transform=lambda *_: trainer.state.iteration,\n",
    ")\n",
    "\n",
    "\n",
    "wandb_logger.attach_opt_params_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_STARTED,\n",
    "    optimizer=optimizer,\n",
    "    param_name='lr'  # optional\n",
    ")\n",
    "\n",
    "# wandb_logger.watch(model) \n",
    "\n",
    "# trainer.run(train_loader, max_epochs=EPOCHS)\n",
    "wandb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea211e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02e1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
